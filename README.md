# IDEAL-PINNs
é’ˆå¯¹ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (PINNs) åœ¨æ±‚è§£é«˜æ¢¯åº¦åå¾®åˆ†æ–¹ç¨‹æ—¶çš„é‡‡æ ·ç‚¹â€œæ¨¡æ€åç¼©â€é—®é¢˜ï¼Œæˆ‘ä»¬ç‹¬ç«‹æå‡ºäº†ä¸€ç§èåˆæœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ä¸é€†å¯†åº¦æ’æ–¥æœºåˆ¶çš„æ–°å‹é‡‡æ ·ç®—æ³•ã€‚ç›¸æ¯”ä¸»æµçš„ RAR/RAD æ–¹æ³•ï¼Œè¯¥ç®—æ³•åœ¨ä¿è¯æ ·æœ¬å¤šæ ·æ€§çš„åŒæ—¶ï¼Œæå‡äº†æ”¶æ•›é€Ÿåº¦ä¸æ±‚è§£ç²¾åº¦ã€‚We designed a novel sampling algorithm to address the "mode collapse" issue in Physics-Informed Neural Networks (PINNs) when solving high-gradient PDEs. By integrating Langevin Dynamics with an Inverse Density Repulsion mechanism,the method show better algorithm sample diversity and significantly faster convergence compared to SOTA baselines (e.g., RAR, RAD).
# IDEAL-PINNs: é¢å‘ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œçš„â€œç†æƒ³â€é‡‡æ ·ç­–ç•¥

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C.svg?style=flat&logo=pytorch)](https://pytorch.org/)
[![Status](https://img.shields.io/badge/Status-Active_Research-blue)]()

> **I**nverse-**D**ensity **E**volutionary **A**daptive **L**angevin Sampling (**IDEAL**)
> (é€†å¯†åº¦æœ—ä¹‹ä¸‡è‡ªé€‚åº”è¿›åŒ–é‡‡æ ·)

**IDEAL-PINNs** æ˜¯ä¸€ä¸ªæ—¨åœ¨åŠ é€Ÿç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (PINNs) æ”¶æ•›çš„æ–°å‹è‡ªé€‚åº”é‡‡æ ·æ¡†æ¶ã€‚å®ƒé€šè¿‡åœ¨**æ¢ç´¢**ï¼ˆé€šè¿‡é€†å¯†åº¦æ’æ–¥ï¼‰å’Œ**åˆ©ç”¨**ï¼ˆé€šè¿‡æœ—ä¹‹ä¸‡æ¢¯åº¦ä¸Šå‡ï¼‰ä¹‹é—´å»ºç«‹**ç†æƒ³å¹³è¡¡**ï¼Œè§£å†³äº†ä¼ ç»Ÿæ®‹å·®é‡‡æ ·æ–¹æ³•ä¸­è‡­åæ˜­è‘—çš„â€œæ¨¡æ€åç¼©â€é—®é¢˜ã€‚

---

## ä¸ºä»€ä¹ˆé€‰æ‹© IDEAL? (ç†è®ºæ¡†æ¶)

æ ‡å‡†çš„è‡ªé€‚åº”ç­–ç•¥ï¼ˆå¦‚ RAD æˆ– RARï¼‰å¾€å¾€éš¾ä»¥ç»´æŒé‡‡æ ·åˆ†å¸ƒ $q(x)$ çš„**ä¿¡æ¯ç†µ** $\mathcal{H}(q)$ï¼Œæ˜“å—**è°±åå·® (Spectral Bias)** å½±å“ã€‚è¿™å¯¼è‡´é‡‡æ ·ç‚¹å…¨éƒ¨èšé›†åœ¨å•ä¸€çš„é«˜è¯¯å·®åŒºåŸŸï¼Œè€Œå¿½ç•¥äº†å…¶ä»–å…³é”®çš„ç‰©ç†ç‰¹å¾ã€‚

**IDEAL** é€šè¿‡æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„**ä¿¡æ¯å¢ç›Š**ç›®æ ‡ï¼Œåœ¨é‡‡æ ·åˆ†å¸ƒ $q(x)$ å’Œç›®æ ‡åˆ†å¸ƒ $\pi(x)$ ä¹‹é—´å»ºç«‹åŠ¨æ€å¹³è¡¡ã€‚

### 1. ç›®æ ‡å‡½æ•°ä¸ç›®æ ‡åˆ†å¸ƒ

PINN è®­ç»ƒçš„ç›®æ ‡æ˜¯æœ€å°åŒ–å®šä¹‰åŸŸ $\Omega$ ä¸Šæ®‹å·®æŸå¤±çš„æœŸæœ›ï¼š
$$
\mathcal{L}_{PINN} = \mathbb{E}_{x \sim q(x)} [\mathcal{L}_{r}(x)] + \mathcal{L}_{bc} + \mathcal{L}_{ic}
$$

æˆ‘ä»¬å®šä¹‰ä¸€ä¸ª**ç›®æ ‡é‡‡æ ·åˆ†å¸ƒ** $\pi(x)$ï¼Œå®ƒä¸ PDE æ®‹å·®çš„æŒ‡æ•°æˆæ­£æ¯”ï¼ˆå°†æ®‹å·®è§†ä¸ºèƒ½é‡ï¼‰ï¼š
$$
\pi(x) \propto \exp(\beta \cdot \mathcal{L}_{r}(x))
$$

### 2. åŸºäºæœ‰æ•ˆåŠ¿èƒ½çš„æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦

æˆ‘ä»¬é‡‡ç”¨**éšæœºæ¢¯åº¦æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ (SGLD)** æ¥æ¼”åŒ–ç²’å­ã€‚ç²’å­çš„è¿åŠ¨éµå¾ª**æœ‰æ•ˆåŠ¿èƒ½** $U_{eff}(x)$ çš„æ¢¯åº¦ï¼š
$$
d x(t) = - \nabla U_{eff}(x) dt + \sqrt{2\eta} d W(t)
$$

å…¶ä¸­ï¼Œ**ç‰©ç†åŠ¿èƒ½**å®šä¹‰ä¸ºè´Ÿæ®‹å·®ï¼ˆå¼•å¯¼ç²’å­å‘é«˜ Loss åŒºåŸŸç§»åŠ¨ï¼‰ï¼š
$$
U_{phys}(x) = - \mathcal{L}_{r}(x) \implies - \nabla U_{phys}(x) = \nabla \mathcal{L}_{r}(x)
$$

### 3. é€†å¯†åº¦æ’æ–¥åŠ¿èƒ½

ä¸ºäº†é˜²æ­¢æ¨¡æ€åç¼©ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªåŸºäºå½“å‰ç²’å­å¯†åº¦ $\rho(x)$ çš„**æ’æ–¥åŠ¿èƒ½**ï¼š
$$
U_{rep}(x) = \lambda \cdot \log(\rho(x))
$$

### 4. IDEAL è”åˆåŠ¨æ€æ–¹ç¨‹

é€šè¿‡å åŠ ç‰©ç†å¸å¼•åŠ›å’Œå¯†åº¦æ’æ–¥åŠ›ï¼Œæˆ‘ä»¬æ¨å¯¼å‡º **IDEAL éšæœºå¾®åˆ†æ–¹ç¨‹ (SDE)**ï¼š
$$
d x(t) = \underbrace{\nabla \mathcal{L}_{r}(x) dt}_{\text{ç‰©ç†å¸å¼•åŠ›}} - \underbrace{\lambda \nabla \log(\rho(x)) dt}_{\text{å¯†åº¦æ’æ–¥åŠ›}} + \underbrace{\sqrt{2\eta} d W(t)}_{\text{éšæœºæ¢ç´¢}}
$$

è¯¥æ–¹ç¨‹ç¡®ä¿ç²’å­çš„ç¨³æ€åˆ†å¸ƒèƒ½å¤Ÿè¦†ç›–æ‰€æœ‰é«˜æ®‹å·®åŒºåŸŸï¼Œè€Œéåç¼©åˆ°å•ä¸€æå€¼ç‚¹ã€‚ã€‚

$$
\text{æœ€ç»ˆé€‰æ‹©æ¦‚ç‡: } \quad P(x) \propto \frac{\mathcal{L}(x)^\alpha}{\rho(x) + \epsilon}
$$

---

## ğŸš€ æ ¸å¿ƒç‰¹æ€§

* **âš¡ è‡ªåŠ¨å¾®åˆ†é©±åŠ¨ (Auto-grad Powered)**: åˆ©ç”¨ PyTorch çš„è‡ªåŠ¨å¾®åˆ†è¿›è¡Œç²¾ç¡®ã€ä¸ç»´åº¦æ— å…³çš„æ¢¯åº¦è®¡ç®—ï¼ˆå½»åº•æ‘’å¼ƒäº†ä½æ•ˆä¸”æœ‰è¯¯å·®çš„æœ‰é™å·®åˆ†ï¼‰ã€‚
* **ğŸŒŠ æ¶ˆé™¤æ¨¡æ€åç¼©**: å¼•å…¥æ’æ–¥æ€§å¯†åº¦é¡¹ï¼Œä¿è¯é‡‡æ ·å™¨èƒ½åŒæ—¶è¦†ç›–å¤šä¸ªè¯¯å·®å³°å€¼ï¼Œé˜²æ­¢â€œæŠ±å›¢â€ç°è±¡ã€‚
* **ğŸ§  æ—¶é—´ä¸€è‡´æ€§**: éå¸¸é€‚åˆæ±‚è§£æ—¶å˜ PDEï¼ˆå¦‚ Allen-Cahn, Navier-Stokesï¼‰ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¿½è¸ªéšæ—¶é—´ç§»åŠ¨çš„è§£ç‰¹å¾ï¼ˆå¦‚æ³¢å‰ï¼‰ã€‚
* **ğŸ“¦ å¼€ç®±å³ç”¨**: è®¾è®¡ä¸ºæ ‡å‡† PINN æ•°æ®åŠ è½½å™¨çš„ç›´æ¥æ›¿ä»£å“ã€‚

---

## ğŸ“Š æ€§èƒ½è¡¨ç°

| æ–¹æ³• | æ ¸å¿ƒæœºåˆ¶ | æ”¶æ•›é€Ÿåº¦ | æ ·æœ¬å¤šæ ·æ€§ | è®¡ç®—æˆæœ¬ |
| :--- | :--- | :--- | :--- | :--- |
| **Uniform** (å‡åŒ€é‡‡æ ·) | éšæœºæ’’ç‚¹ | æ…¢ | é«˜ | ä½ |
| **RAD** (DeepXDE) | åŸºäº Loss åŠ æƒ | ä¸­ç­‰ | ä½ (æ˜“èšé›†) | ä½ |
| **RAR** (Refinement) | å¤§æ± å­è´ªå¿ƒç­›é€‰ | æ…¢ | ä¸­ç­‰ | é«˜ ($10\times$) |
| **IDEAL (Ours)** | **è¿›åŒ– + æ’æ–¥** | **æå¿«** | **é«˜ (è‡ªé€‚åº”)** | **ä¸­ç­‰** |

> *[å›¾ 1: IDEAL é‡‡æ ·ç‚¹åœ¨ 1D Burgers æ–¹ç¨‹ä¸Šçš„æ¼”åŒ–è¿‡ç¨‹ã€‚çº¢ç‚¹ä»£è¡¨é‡‡æ ·ç‚¹ï¼Œå®Œç¾åœ°è¿½è¸ªå¹¶è¦†ç›–äº†æ¿€æ³¢é”‹é¢ã€‚]*

---

## å®‰è£…ä¸ä½¿ç”¨

### 1. ä¾èµ–ç¯å¢ƒ
```bash
pip install numpy torch scipy matplotlib
```

### 2. å¿«é€Ÿä¸Šæ‰‹
åªéœ€å¯¼å…¥ `IDEALSampler` å¹¶åœ¨ä½ çš„è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨å³å¯ã€‚

```python
import torch
from ideal_sampler import IDEALSampler  # ä½ çš„æ ¸å¿ƒæ–‡ä»¶

# --- 1. å®šä¹‰ PDE æ®‹å·® (Loss å‡½æ•°) ---
def pde_residual(xy):
    """
    xy: [Batch, Dimension] tensor, requires_grad=True
    è¿”å›: [Batch, 1] æ®‹å·®çš„å¹³æ–¹
    """
    # ç¤ºä¾‹: u_t + u*u_x = \nu * u_xx
    u = model(xy)
    # ... è®¡ç®—æ¢¯åº¦å’Œç‰©ç†æ®‹å·® ...
    residual = ... 
    return residual ** 2

# --- 2. åˆå§‹åŒ– IDEAL é‡‡æ ·å™¨ ---
sampler = IDEALSampler(
    loss_fn=pde_residual,
    domain_bounds=[[-1.0, 1.0], [0.0, 1.0]], # æ—¶ç©ºå®šä¹‰åŸŸ
    device='cuda',
    use_autograd=True  # å¼€å¯è‡ªåŠ¨å¾®åˆ†ä»¥è·å¾—é«˜ç²¾åº¦ä¸æ‰©å±•æ€§
)

# --- 3. è®­ç»ƒå¾ªç¯ ---
for epoch in range(max_epochs):
    # è·å–è‡ªé€‚åº”é‡‡æ ·ç‚¹ (è¿›åŒ–è¿‡ç¨‹åœ¨å†…éƒ¨è‡ªåŠ¨å®Œæˆ)
    # é‡‡æ ·å™¨ä¼šè‡ªåŠ¨ç®¡ç†ç§ç¾¤è®°å¿†
    x_train = sampler.get_samples(n_samples=2000)
    
    # æ ‡å‡†çš„ PINN æ›´æ–°æ­¥éª¤
    loss = train_step(x_train)
    
    if epoch % 100 == 0:
        print(f"Epoch {epoch}: Loss {loss.item():.5f}")
```

---

## å¼€å‘è·¯çº¿å›¾ (Roadmap)

- **æ ¸å¿ƒç®—æ³•**: å®ç°åŸºäºç½‘æ ¼å¯†åº¦ä¼°è®¡çš„ IDEAL é‡‡æ ·ã€‚
- **æ€§èƒ½ä¼˜åŒ–**: å®ç°åŸºäºè‡ªåŠ¨å¾®åˆ† (Auto-grad) çš„æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦ã€‚
- **é«˜ç»´æ‰©å±•**: å®ç°åŸºäº KNN çš„æ— ç½‘æ ¼å¯†åº¦ä¼°è®¡ï¼Œä»¥æ”¯æŒé«˜ç»´ (>3D) PDEã€‚
- **åŸºå‡†æµ‹è¯•**: åœ¨ Navier-Stokes æ–¹ç¨‹ä¸Šä¸ SVGD å’Œ Failure-Informed Sampling è¿›è¡Œå…¨é¢å¯¹æ¯”ã€‚

---

## å¼•ç”¨ (Citation)

å¦‚æœæ‚¨è§‰å¾— **IDEAL-PINNs** å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘å¼•ç”¨ï¼š

```bibtex
@misc{ideal_pinn_2025,
  author = {Xu Yang},
  title = {IDEAL-PINNs: Inverse-Density Evolutionary Adaptive Langevin Sampling},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{[https://github.com/XuYang-06/IDEAL-PINNs](https://github.com/XuYang-06/IDEAL-PINNs)}}
}
```

---


## ğŸ‘¤ ä½œè€… (Authors)

| [å¾é˜³ (Xu Yang)](https://www.google.com/search?q=https://github.com/XuYang-06) | é™ˆå®æ¶› (Hongtao Chen) | æ¨ä½³å® (Jianing Yang) | éƒ­ç§‘æ–‡ (Kewen Guo) |
| :---: | :---: | :---: | :---: |
| *å‰æ—å¤§å­¦* | *å‰æ—å¤§å­¦* | *å‰æ—å¤§å­¦* | *å‰æ—å¤§å­¦* |
| CV, PINN, å…·èº«æ™ºèƒ½ | æ·±åº¦å­¦ä¹ , PINN | æ·±åº¦å­¦ä¹ , PINN | æ·±åº¦å­¦ä¹ , PINN |

---
